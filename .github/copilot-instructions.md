
# Copilot / AI agent instructions for ETL Test Framework

Purpose: give AI coding agents immediate, actionable knowledge to make safe, useful edits.

1) Big picture
- This is a lightweight ETL testing framework: test cases and expectations live in an Excel workbook (`control/etl_test_control.xlsx`) and are read by `utils/excel_utils.py`.
- Tests are executed by `core.TestExecutor` (`core/test_executor.py`) which: obtains readers via `core/reader_factory.py`, reads source/target data, runs count/dup/incremental checks and applies expectation rules from `read_expectations()`.
- Parallel execution is handled by `core.parallel_executor.run_parallel` (ThreadPoolExecutor). Reports are generated by `reports.html_reporter.generate_html` and written to `reports/etl_report.html`.

2) Key files (roles)
- `run_regression.py` / `run_smoke.py`: entry scripts used by developers/CI to run suites.
- `utils/excel_utils.py`: canonical source of truth for test-case and expectation schemas.
- `core/test_executor.py`: central business logic and RULE_REGISTRY for expectation rules.
- `core/reader_factory.py`: maps a `reader_type` (CSV/DB) to a reader implementation.
- `readers/csv_reader.py`, `readers/db_reader.py`: concrete readers. DB reader loads `config/db_config.yaml` and uses SQLAlchemy.

3) Concrete, discoverable conventions and patterns
- Test case dict shape (derived from `read_test_cases()` + usage in `TestExecutor.execute`): must include keys: `test_id`, `table_name`, `run_type` (`SMOKE`|`REGRESSION`), `source_type`, `source_details`, `target_type`, `target_details`, `primary_key`, optional `incremental_column`.
- Expectations sheet rows map to `rule_type` values found in `RULE_REGISTRY` (see `core/test_executor.py`). Add new expectation functions and register them in `RULE_REGISTRY`.
- Readers: `get_reader(reader_type, details)` expects `reader_type` like `CSV` or `DB` and `details` matching the reader constructor (CSV: path; DB: table name). To add a new reader, implement `BaseReader` subclass in `readers/` and update `core/reader_factory.py`.
- DB connections: `readers/db_reader.py` reads `config/db_config.yaml` with top-level keys for named DBs; connection string used by SQLAlchemy `create_engine`.

4) Development & run workflows
- Create venv and install: `python -m venv venv` then `source venv/bin/activate` and `pip install -r requirements.txt` (also listed in `README.md`).
- Run full regression: `python run_regression.py`; smoke: `python run_smoke.py`.
- Reports are written by `reports/html_reporter.py` to `reports/etl_report.html`.

5) Safety and testing guidance for AI edits (project-specific)
- Changing `RULE_REGISTRY` or `TestExecutor.execute` affects all tests â€” prefer adding a new rule and tests rather than editing existing rule semantics.
- When adding a reader, update `core/reader_factory.py` and ensure `read()` returns a pandas DataFrame.
- Avoid changing Excel sheet names or column names in `control/etl_test_control.xlsx` unless you also update `utils/excel_utils.py` and any downstream code that relies on those fields.

6) Examples (copy-paste friendly)
- To add a rule:
  - implement `def expect_new_rule(df, column, value): ...` in `core/test_executor.py` or a new helper module
  - add to `RULE_REGISTRY["new_rule"] = expect_new_rule`
- To add a CSV reader for a non-standard delimiter, create `readers/my_csv_reader.py` subclassing `BaseReader`, implement `read()` using `pd.read_csv(..., sep=';')`, and register it in `core/reader_factory.py`.

7) Useful places to check when debugging
- Data inputs: `data/` (sample CSVs) and `control/etl_test_control.xlsx` (test definitions)
- DB config: `config/db_config.yaml`
- Execution: `run_regression.py` / `run_smoke.py`
- Core logic and rules: `core/test_executor.py`

If anything above is unclear or you'd like a different structure (more examples, CI guidance, or unit-test patterns), tell me what to expand and I'll iterate.
